<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Aligned Probing: Relating Toxic Behavior and Model Internals">
  <meta name="keywords" content="Aligned Probing, Probing, Language Models, Language Models Behavior, Language Models Interpretability">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Aligned Probing: Relating Toxic Behavior and Model Internals</title>
  <meta name="google-site-verification" content="vFzufXia5FHJ-451nWxxMaAAsjMWZcY5Cn6NoZYsgeg" />
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://unpkg.com/flickity@2/dist/flickity.min.css">
  <script src="https://unpkg.com/flickity@2/dist/flickity.pkgd.min.js"></script>


  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Aligned Probing: Relating Toxic Behavior and Model Internals</h1>
          <!--  <div class="is-size-5 publication-authors">
             <span class="author-block">
               <a href="https://keunhong.com">Andreas Waldis</a><sup>1</sup>,</span>
             <span class="author-block">
               <a href="https://utkarshsinha.com"></a><sup>2</sup>,</span>
             <span class="author-block">
               <a href="https://jonbarron.info">Jonathan T. Barron</a><sup>2</sup>,
             </span>
             <span class="author-block">
               <a href="http://sofienbouaziz.com">Sofien Bouaziz</a><sup>2</sup>,
             </span>
             <span class="author-block">
               <a href="https://www.danbgoldman.com">Dan B Goldman</a><sup>2</sup>,
             </span>
             <span class="author-block">
               <a href="http   s://homes.cs.washington.edu/~seitz/">Steven M. Seitz</a><sup>1,2</sup>,
             </span>
             <span class="author-block">
               <a href="http://www.ricardomartinbrualla.com">Ricardo Martin-Brualla</a><sup>2</sup>
             </span>
           </div>

           <div class="is-size-5 publication-authors">
             <span class="author-block"><sup>1</sup>University of Washington,</span>
             <span class="author-block"><sup>2</sup>Google Research</span>
           </div>-->

           <div class="column has-text-centered">
             <div class="publication-links">
               <!-- PDF Link. -->
              <span class="link-block">
                <a href="aligned_probing.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
               <!-- <span class="link-block">
                 <a href="https://arxiv.org/abs/2011.12948"
                    class="external-link button is-normal is-rounded is-dark">
                   <span class="icon">
                       <i class="ai ai-arxiv"></i>
                   </span>
                   <span>arXiv</span>
                 </a>
               </span>

              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>
             -->

             <span class="link-block">
                <a href="https://github.com/alignedprobing/aligned-probing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
             </div>
        </div>
      </div>
    </div>
  </div>
</section>

<hr>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Language models (LMs) can generate toxic language even when given non-toxic prompts.
            Previous research has primarily analyzed this behavior by assessing the toxicity of generated text, but largely overlooked how models internally process toxicity.
          </p>
          <p>
            We introduce <strong>Aligned Probing</strong>, a method that traces toxic language from the <strong class="blue">input</strong> through all model layers to the generated
            <strong class="yellow">output</strong>. This approach provides a more comprehensive evaluation of toxicity in LMs by aligning
            and quantifying where and how strongly models encode toxic information. Additionally, we treat toxicity as a heterogeneous phenomenon and analyze six fine-grained attributes, such as <i>Threat</i> and <i>Identity Attack</i>,
            as defined by the <a href="https://perspectiveapi.com/" class="purple"><u>PERSPECTIVE API</u></a>.
          </p>
          <p>
            Applying <strong>Aligned Probing</strong> to 20+ models, including Llama, OLMo, and Mistral,
            reveals that LMs tend to encode toxic language most strongly in lower layers, with distinct patterns
            across different toxicity attributes. Furthermore, our findings suggest that less toxic models encode more information about the toxicity of the <strong class="blue">input</strong>.
          </p>
        </div>
      </div>
    </div>

  </div>
</section>


<section class="section hero is-light">
  <div class="">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/aligned_probing.svg" width="5%" alt="Icon">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3">How <u>Aligned Probing</u> works</h2>
        <div class="content has-text-justified">

          <strong>Aligned Probing</strong> consists of four steps to examine the behavior and internal processes of LMs, connecting these perspectives to analyze their interplay in the context of toxicity.

          <ul>
            <li><strong>Inference:</strong> An LM generates text based on a given <strong class="blue">input</strong> prompt. Simultaneously, we collect internal representations from every model layer for both the <strong class="blue">input</strong> and the generated <strong class="yellow">output</strong>.</li>

            <li><strong>Toxic Behavior:</strong> We assess the toxicity of the <strong class="blue">input</strong> and the generated <strong class="yellow">output</strong> using the
              <a href="https://perspectiveapi.com/" class="purple"><u>PERSPECTIVE API</u></a>.</li>

            <li><strong>Encoding of Toxic Language in LMs:</strong> We use linear models (probes) to evaluate how LMs encode information about toxicity at each model layer across four distinct scenarios:
              <ul>
                <li><strong><u>Input</u> scenario:</strong> Examines how LMs capture the toxicity of the <strong class="blue">input</strong> within their <strong class="blue">input</strong> internals.</li>
                <li><strong><u>Forward</u> scenario:</strong> Analyzes how LMs propagate information about <strong class="blue">input</strong> toxicity within their <strong class="yellow">output</strong> internals.</li>
                <li><strong><u>Output</u> scenario:</strong> Investigates how strongly LMs encode the toxicity of the generated <strong class="yellow">output</strong> in their <strong class="yellow">output</strong> internals.</li>
                <li><strong><u>Backward</u> scenario:</strong> Studies how much information about <strong class="yellow">output</strong> toxicity is retained within the <strong class="blue">input</strong> internals.</li>
              </ul>
            </li>

            <li><strong>Interplay:</strong> We correlate the behavioral and internal evaluations to analyze how these perspectives relate. Through layer-wise interventions, we causally validate these insights.</li>
          </ul>
        </div>

      </div>
    </div>

    <div class="slideshow-container">
      <img src="static/images/aligned_probing_step_1.svg" alt="Step 1">
      <img src="static/images/aligned_probing_step_2.svg" alt="Step 2">
      <img src="static/images/aligned_probing_step_3.svg" alt="Step 3">
      <img src="static/images/aligned_probing_step_4.svg" alt="Step 4">
    </div>
  </div>
</section>



<section class="section hero is-light">
  <div class="">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/aligned_probing.svg" width="5%" alt="Icon">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">

      <div class="column is-four-fifths">
        <h2 class="title is-3">Key Findings</h2>
        <div class="content has-text-justified">

        <p>
          <h5 class="is-5">i) Lower layers strongly encode toxic language.</h5>
          <div>
            LMs encode toxic language most strongly in their lower layers. Particularly, we find the most information for the <strong class="blue">input</strong> toxicity within <strong class="blue">input</strong> internals (Scenario Input).
            This peak varies depending on the toxicity attribute. More contextualized attributes (such as <i>Threat</i>) peak in higher layers, whereas we find the highest information for attributes more sensitive to individual words, like <i>Sexually Explicit</i>, in lower layers.
          </div>
<br>          <br>

          <img src="static/images/results_1.png" alt="Results 1">
        </p>
          <br>
          <p>
            <h5 class="is-5">ii) Instruction-tuning changes how information is encoded.</h5>
          <div>
            Comparing pre-trained and instruction-tuned LMs shows that instruction-tuning increases information about <strong class="blue">input</strong> toxicity while reducing information about the <strong class="yellow">output</strong> toxicity.
            Interestingly, this effect is more pronounced for toxicity attributes that are less sensitive to single words, such as <i>Threat</i>, hinting at the contextualized impact of instruction-tuning.
          </div>
 <br>          <br>
            <img src="static/images/results_2.png" alt="Results 2">
          </p>
          <br>
          <p>
            <h5 class="is-5">iii) Less toxic öanguage models encode more information about input toxicity.</h5>
           <div>
             Analyzing the relationship between toxic behavior and how LMs encode toxic language (<i>left</i>) shows that models encoding more information about the <strong class="blue">input</strong> toxicity tend to produce less toxic outputs.
             Simultaneously, more toxic models also encode more information about the toxicity of the generated <strong class="yellow">output</strong>.
             We further find that this correlation has a causal component: intervening on single model layers to simulate the removal of toxicity-related information increases the toxicity of the model's output (<i>right</i>).
           </div>
            <br>          <br>
            <img src="static/images/results_3.png" alt="Results 3">
          </p>

      </div>
    </div>
  </div>
</section>



<section class="section hero is-light">
  <div class="">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <img src="static/images/aligned_probing.svg" width="5%" alt="Icon">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The Dilemma of Generative LMs</h2>


        <div class="content has-text-justified">
          Using <strong>Aligned Probing</strong>, we found that model behavior is heavily influenced by the toxicity of the <strong class="blue">input</strong>, and that model internals strongly encode and propagate this toxicity information.
          This substantial dependence on input properties highlights a fundamental dilemma in generative models: while we expect them to produce semantically relevant outputs based on a given prompt, they should ideally ignore unwanted attributes such as toxicity.
          Moving toward more controllable text generation, we plan to extend <strong>Aligned Probing</strong> to analyze additional aspects of language modeling, such as stereotypical formulations, and to explore the effectiveness of other mitigation strategies, including model merging.
        </div>
        </div>
    </div>

  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/alignedprobing/alignedprobing.github.io/">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
